{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Build a linear regression model with interactions and polynomial features \n",
    "- Use feature selection to obtain the optimal subset of features in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Started!\n",
    "\n",
    "Below we import all the necessary packages for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"ames.csv\")\n",
    "# Subset columns\n",
    "df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n",
    "         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n",
    "         'GarageArea', 'Fireplaces', 'SalePrice']]\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df['SalePrice']\n",
    "X = df.drop(columns='SalePrice')\n",
    "\n",
    "# Split into train, test, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Housing Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we imported the Ames housing data and grabbed a subset of the data to use in this analysis.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Scale all the predictors using `StandardScaler`, then convert these scaled features back into DataFrame objects\n",
    "- Build a baseline `LinearRegression` model using *scaled variables* as predictors and use the $R^2$ score to evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Scale X_train and X_val using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Ensure X_train_scaled and X_val_scaled are DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline R² score on training data: 0.7868\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 2: Create a LinearRegression model and fit it on scaled training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate a baseline r-squared score on training data\n",
    "r2_train = model.score(X_train_scaled, y_train)\n",
    "print(f\"Baseline R² score on training data: {r2_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Interactions\n",
    "\n",
    "Instead of adding all possible interaction terms, let's try a custom technique. We are only going to add the interaction terms that increase the $R^2$ score as much as possible. Specifically we are going to look for the 7 interaction terms that each cause the most increase in the coefficient of determination.\n",
    "\n",
    "### Find the Best Interactions\n",
    "\n",
    "Look at all the possible combinations of variables for interactions by adding interactions one by one to the baseline model. Create a data structure that stores the pair of columns used as well as the $R^2$ score for each combination.\n",
    "\n",
    "***Hint:*** We have imported the `combinations` function from `itertools` for you ([documentation here](https://docs.python.org/3/library/itertools.html#itertools.combinations)). Try applying this to the columns of `X_train` to find all of the possible pairs.\n",
    "\n",
    "Print the 7 interactions that result in the highest $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: LotArea * 1stFlrSF, R² Score: 0.7211\n",
      "Interaction: LotArea * TotalBsmtSF, R² Score: 0.7072\n",
      "Interaction: LotArea * GrLivArea, R² Score: 0.6691\n",
      "Interaction: LotArea * Fireplaces, R² Score: 0.6530\n",
      "Interaction: 2ndFlrSF * TotRmsAbvGrd, R² Score: 0.6473\n",
      "Interaction: OverallCond * TotalBsmtSF, R² Score: 0.6429\n",
      "Interaction: OverallQual * 2ndFlrSF, R² Score: 0.6422\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 1: Set up data structure to store interaction pairs and their R² scores\n",
    "interaction_results = []\n",
    "\n",
    "# Step 2: Find combinations of columns and loop over them\n",
    "for col1, col2 in combinations(X.columns, 2):\n",
    "    # Make copies of X_train and X_val\n",
    "    X_train_interaction = X_train.copy()\n",
    "    X_val_interaction = X_val.copy()\n",
    "    \n",
    "    # Add interaction term to data\n",
    "    X_train_interaction[f'{col1}_x_{col2}'] = X_train_interaction[col1] * X_train_interaction[col2]\n",
    "    X_val_interaction[f'{col1}_x_{col2}'] = X_val_interaction[col1] * X_val_interaction[col2]\n",
    "    \n",
    "    # Scale the new features\n",
    "    X_train_interaction_scaled = scaler.fit_transform(X_train_interaction)\n",
    "    X_val_interaction_scaled = scaler.transform(X_val_interaction)\n",
    "    \n",
    "    # Fit the model on the new training data\n",
    "    model.fit(X_train_interaction_scaled, y_train)\n",
    "    \n",
    "    # Find r-squared score (fit on training data, evaluate on validation data)\n",
    "    r2_score = model.score(X_val_interaction_scaled, y_val)\n",
    "    \n",
    "    # Append to data structure\n",
    "    interaction_results.append((col1, col2, r2_score))\n",
    "\n",
    "# Step 3: Sort and subset the data structure to find the top 7\n",
    "interaction_results.sort(key=lambda x: x[2], reverse=True)\n",
    "top_interactions = interaction_results[:7]\n",
    "\n",
    "# Print the top 7 interactions and their R² scores\n",
    "for interaction in top_interactions:\n",
    "    print(f\"Interaction: {interaction[0]} * {interaction[1]}, R² Score: {interaction[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Interactions\n",
    "\n",
    "Write code to include the 7 most important interactions in `X_train` and `X_val` by adding 7 columns. Use the naming convention `\"col1_col2\"`, where `col1` and `col2` are the two columns in the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated X_train columns: ['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageArea', 'Fireplaces', 'LotArea_1stFlrSF', 'LotArea_TotalBsmtSF', 'LotArea_GrLivArea', 'LotArea_Fireplaces', '2ndFlrSF_TotRmsAbvGrd', 'OverallCond_TotalBsmtSF', 'OverallQual_2ndFlrSF']\n",
      "Updated X_val columns: ['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageArea', 'Fireplaces', 'LotArea_1stFlrSF', 'LotArea_TotalBsmtSF', 'LotArea_GrLivArea', 'LotArea_Fireplaces', '2ndFlrSF_TotRmsAbvGrd', 'OverallCond_TotalBsmtSF', 'OverallQual_2ndFlrSF']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Loop over top 7 interactions\n",
    "for interaction in top_interactions:\n",
    "    # Extract column names from data structure\n",
    "    col1, col2, _ = interaction  # We don't need the R² score here\n",
    "    \n",
    "    # Construct new column name\n",
    "    new_column_name = f\"{col1}_{col2}\"\n",
    "    \n",
    "    # Add new column to X_train and X_val\n",
    "    X_train[new_column_name] = X_train[col1] * X_train[col2]\n",
    "    X_val[new_column_name] = X_val[col1] * X_val[col2]\n",
    "\n",
    "# Optional: Display the updated X_train and X_val to verify the new columns\n",
    "print(\"Updated X_train columns:\", X_train.columns.tolist())\n",
    "print(\"Updated X_val columns:\", X_val.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Polynomials\n",
    "\n",
    "Now let's repeat that process for adding polynomial terms.\n",
    "\n",
    "### Find the Best Polynomials\n",
    "\n",
    "Try polynomials of degrees 2, 3, and 4 for each variable, in a similar way you did for interactions (by looking at your baseline model and seeing how $R^2$ increases). Do understand that when going for a polynomial of degree 4 with `PolynomialFeatures`, the particular column is raised to the power of 2 and 3 as well in other terms.\n",
    "\n",
    "We only want to include \"pure\" polynomials, so make sure no interactions are included.\n",
    "\n",
    "Once again you should make a data structure that contains the values you have tested. We recommend a list of tuples of the form:\n",
    "\n",
    "`(col_name, degree, R2)`, so eg. `('OverallQual', 2, 0.781)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fitting model for LotArea^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for LotArea^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for LotArea^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallQual^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallQual^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallQual^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallCond^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallCond^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for OverallCond^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotalBsmtSF^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotalBsmtSF^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotalBsmtSF^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 1stFlrSF^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 1stFlrSF^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 1stFlrSF^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 2ndFlrSF^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 2ndFlrSF^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for 2ndFlrSF^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GrLivArea^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GrLivArea^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GrLivArea^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotRmsAbvGrd^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotRmsAbvGrd^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for TotRmsAbvGrd^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GarageArea^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GarageArea^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for GarageArea^4: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for Fireplaces^2: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for Fireplaces^3: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "Error fitting model for Fireplaces^4: Input contains NaN, infinity or a value too large for dtype('float64').\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 1: Set up data structure to store polynomial terms and their R² scores\n",
    "polynomial_results = []\n",
    "\n",
    "# Check for NaN values in X_train and X_val\n",
    "if X_train.isnull().values.any() or X_val.isnull().values.any():\n",
    "    print(\"Warning: NaN values found in the dataset.\")\n",
    "    # Fill NaN values with the mean of each column\n",
    "    X_train.fillna(X_train.mean(), inplace=True)\n",
    "    X_val.fillna(X_val.mean(), inplace=True)\n",
    "\n",
    "# Check for infinite values in X_train and X_val\n",
    "if not np.isfinite(X_train.values).all() or not np.isfinite(X_val.values).all():\n",
    "    print(\"Warning: Infinite values found in the dataset.\")\n",
    "    # Handle infinite values if necessary (e.g., replace with finite values)\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_val.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Optionally fill NaN values again after replacing infinite values\n",
    "    X_train.fillna(X_train.mean(), inplace=True)\n",
    "    X_val.fillna(X_val.mean(), inplace=True)\n",
    "\n",
    "# Step 2: Loop over all columns\n",
    "for col in X.columns:\n",
    "    # Loop over degrees 2, 3, 4\n",
    "    for degree in range(2, 5):\n",
    "        # Make a copy of X_train and X_val\n",
    "        X_train_poly = X_train.copy()\n",
    "        X_val_poly = X_val.copy()\n",
    "        \n",
    "        # Instantiate PolynomialFeatures with relevant degree\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        \n",
    "        # Fit polynomial to column and transform column\n",
    "        poly_features_train = poly.fit_transform(X_train_poly[[col]])\n",
    "        poly_features_val = poly.transform(X_val_poly[[col]])\n",
    "        \n",
    "        # Convert the result to DataFrame\n",
    "        poly_features_train_df = pd.DataFrame(poly_features_train, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "        poly_features_val_df = pd.DataFrame(poly_features_val, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "        \n",
    "        # Add polynomial to data\n",
    "        # Drop the original column before concatenating\n",
    "        X_train_poly = pd.concat([X_train_poly.drop(columns=[col]), poly_features_train_df], axis=1)\n",
    "        X_val_poly = pd.concat([X_val_poly.drop(columns=[col]), poly_features_val_df], axis=1)\n",
    "        \n",
    "        # Fit the model on the new training data\n",
    "        try:\n",
    "            model.fit(X_train_poly, y_train)\n",
    "            # Find r-squared score on validation\n",
    "            r2_score = model.score(X_val_poly, y_val)\n",
    "            # Append to data structure\n",
    "            polynomial_results.append((col, degree, r2_score))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error fitting model for {col}^{degree}: {e}\")\n",
    "\n",
    "# Step 3: Sort and subset the data structure to find the top 7\n",
    "polynomial_results.sort(key=lambda x: x[2], reverse=True)\n",
    "top_polynomials = polynomial_results[:7]\n",
    "\n",
    "# Print the top 7 polynomial terms and their R² scores\n",
    "for polynomial in top_polynomials:\n",
    "    print(f\"Polynomial: {polynomial[0]}^{polynomial[1]}, R² Score: {polynomial[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Best Polynomials\n",
    "\n",
    "If there are duplicate column values in the results above, don't add multiple of them to the model, to avoid creating duplicate columns. (For example, if column `A` appeared in your list as both a 2nd and 3rd degree polynomial, adding both would result in `A` squared being added to the features twice.) Just add in the polynomial that results in the highest R-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out duplicates, keeping only the best polynomial for each column\n",
    "best_polynomials = {}\n",
    "for col, degree, r2 in polynomial_results:\n",
    "    if col not in best_polynomials or r2 > best_polynomials[col][1]:\n",
    "        best_polynomials[col] = (degree, r2)\n",
    "\n",
    "# Step 2: Create a list to store new polynomial features\n",
    "new_polynomial_features_train = []\n",
    "new_polynomial_features_val = []\n",
    "\n",
    "# Step 3: Loop over remaining results to create polynomial terms\n",
    "for col, (degree, r2) in best_polynomials.items():\n",
    "    # Instantiate PolynomialFeatures with the best degree\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Create polynomial features for training and validation sets\n",
    "    poly_features_train = poly.fit_transform(X_train[[col]])\n",
    "    poly_features_val = poly.transform(X_val[[col]])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    poly_features_train_df = pd.DataFrame(poly_features_train, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    poly_features_val_df = pd.DataFrame(poly_features_val, columns=[f\"{col}^{d}\" for d in range(1, degree + 1)])\n",
    "    \n",
    "    # Append new polynomial features to the list\n",
    "    new_polynomial_features_train.append(poly_features_train_df)\n",
    "    new_polynomial_features_val.append(poly_features_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your final data set and make sure that your interaction terms as well as your polynomial terms are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X_train shape: (821, 17)\n",
      "Final X_val shape: (274, 17)\n",
      "X_train head:\n",
      "       LotArea  OverallQual  OverallCond  TotalBsmtSF  1stFlrSF  2ndFlrSF  \\\n",
      "518      9531            6            5          794       882       914   \n",
      "236      8773            7            5         1414      1414         0   \n",
      "38       7922            5            7         1057      1057         0   \n",
      "1034     6305            5            7          920       954         0   \n",
      "520     10800            4            7            0       694       600   \n",
      "\n",
      "      GrLivArea  TotRmsAbvGrd  GarageArea  Fireplaces  LotArea_1stFlrSF  \\\n",
      "518        1796             7         546           0           8406342   \n",
      "236        1414             6         494           0          12405022   \n",
      "38         1057             5         246           0           8373554   \n",
      "1034        954             5         240           1           6014970   \n",
      "520        1294             7           0           0           7495200   \n",
      "\n",
      "      LotArea_TotalBsmtSF  LotArea_GrLivArea  LotArea_Fireplaces  \\\n",
      "518               7567614           17117676                   0   \n",
      "236              12405022           12405022                   0   \n",
      "38                8373554            8373554                   0   \n",
      "1034              5800600            6014970                6305   \n",
      "520                     0           13975200                   0   \n",
      "\n",
      "      2ndFlrSF_TotRmsAbvGrd  OverallCond_TotalBsmtSF  OverallQual_2ndFlrSF  \n",
      "518                    6398                     3970                  5484  \n",
      "236                       0                     7070                     0  \n",
      "38                        0                     7399                     0  \n",
      "1034                      0                     6440                     0  \n",
      "520                    4200                        0                  2400  \n",
      "X_val head:\n",
      "       LotArea  OverallQual  OverallCond  TotalBsmtSF  1stFlrSF  2ndFlrSF  \\\n",
      "1431     4928            6            6          958       958         0   \n",
      "1304     3363            7            5          976       976       732   \n",
      "50      13869            6            6          794       794       676   \n",
      "378     11394            9            2         1856      1856         0   \n",
      "1124     9125            7            5          384       812       670   \n",
      "\n",
      "      GrLivArea  TotRmsAbvGrd  GarageArea  Fireplaces  LotArea_1stFlrSF  \\\n",
      "1431        958             5         440           0           4721024   \n",
      "1304       1708             7         380           0           3282288   \n",
      "50         1470             6         388           0          11011986   \n",
      "378        1856             8         834           1          21147264   \n",
      "1124       1482             7         392           1           7409500   \n",
      "\n",
      "      LotArea_TotalBsmtSF  LotArea_GrLivArea  LotArea_Fireplaces  \\\n",
      "1431              4721024            4721024                   0   \n",
      "1304              3282288            5744004                   0   \n",
      "50               11011986           20387430                   0   \n",
      "378              21147264           21147264               11394   \n",
      "1124              3504000           13523250                9125   \n",
      "\n",
      "      2ndFlrSF_TotRmsAbvGrd  OverallCond_TotalBsmtSF  OverallQual_2ndFlrSF  \n",
      "1431                      0                     5748                     0  \n",
      "1304                   5124                     4880                  5124  \n",
      "50                     4056                     4764                  4056  \n",
      "378                       0                     3712                     0  \n",
      "1124                   4690                     1920                  4690  \n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 4: Concatenate new polynomial features to X_train and X_val\n",
    "if new_polynomial_features_train:\n",
    "    X_train_polynomials = pd.concat(new_polynomial_features_train, axis=1)\n",
    "    X_val_polynomials = pd.concat(new_polynomial_features_val, axis=1)\n",
    "    \n",
    "    # Concatenate with original data\n",
    "    X_train = pd.concat([X_train.reset_index(drop=True), X_train_polynomials.reset_index(drop=True)], axis=1)\n",
    "    X_val = pd.concat([X_val.reset_index(drop=True), X_val_polynomials.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Check the final dataset to ensure polynomial terms are included\n",
    "print(\"Final X_train shape:\", X_train.shape)\n",
    "print(\"Final X_val shape:\", X_val.shape)\n",
    "\n",
    "# Optionally, display the first few rows of the final datasets\n",
    "print(\"X_train head:\\n\", X_train.head())\n",
    "print(\"X_val head:\\n\", X_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the $R^2$ of the full model with your interaction and polynomial terms added. Print this value for both the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R² Score: 0.8004\n",
      "Validation R² Score: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Calculate R² scores for training and validation datasets\n",
    "train_r2_score = model.score(X_train, y_train)\n",
    "val_r2_score = model.score(X_val, y_val)\n",
    "\n",
    "# Step 3: Print the R² scores\n",
    "print(f\"Training R² Score: {train_r2_score:.4f}\")\n",
    "print(f\"Validation R² Score: {val_r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we may be overfitting some now. Let's try some feature selection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "First, test out `RFE` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) with several different `n_features_to_select` values. For each value, print out the train and validation $R^2$ score and how many features remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE - n_features_to_select=5:\n",
      "Training R² Score: 0.7281, Validation R² Score: 0.6817, Features Remaining: 5\n",
      "\n",
      "RFE - n_features_to_select=10:\n",
      "Training R² Score: 0.7896, Validation R² Score: 0.6444, Features Remaining: 10\n",
      "\n",
      "RFE - n_features_to_select=15:\n",
      "Training R² Score: 0.7990, Validation R² Score: 0.7377, Features Remaining: 15\n",
      "\n",
      "RFE - n_features_to_select=20:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n",
      "RFE - n_features_to_select=25:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Define the model for RFE\n",
    "model_rfe = LinearRegression()\n",
    "\n",
    "# Step 1: RFE with different n_features_to_select values\n",
    "n_features_options = [5, 10, 15, 20, 25]  # Adjust these values based on your dataset\n",
    "for n_features in n_features_options:\n",
    "    rfe = RFE(estimator=model_rfe, n_features_to_select=n_features)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    # Get selected features\n",
    "    X_train_rfe = X_train.iloc[:, rfe.support_]\n",
    "    X_val_rfe = X_val.iloc[:, rfe.support_]\n",
    "    \n",
    "    # Fit the model and calculate R² scores\n",
    "    model_rfe.fit(X_train_rfe, y_train)\n",
    "    train_r2_score_rfe = model_rfe.score(X_train_rfe, y_train)\n",
    "    val_r2_score_rfe = model_rfe.score(X_val_rfe, y_val)\n",
    "    \n",
    "    print(f\"RFE - n_features_to_select={n_features}:\")\n",
    "    print(f\"Training R² Score: {train_r2_score_rfe:.4f}, Validation R² Score: {val_r2_score_rfe:.4f}, Features Remaining: {X_train_rfe.shape[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test out `Lasso` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)) with several different `alpha` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - alpha=0.01:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n",
      "Lasso - alpha=0.1:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n",
      "Lasso - alpha=1:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n",
      "Lasso - alpha=10:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n",
      "Lasso - alpha=100:\n",
      "Training R² Score: 0.8004, Validation R² Score: 0.7468, Features Remaining: 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100]  # Adjust these values based on your dataset\n",
    "for alpha in alpha_values:\n",
    "    lasso = LassoCV(alphas=[alpha], cv=5)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features_lasso = np.where(lasso.coef_ != 0)[0]  # Indices of selected features\n",
    "    X_train_lasso = X_train.iloc[:, selected_features_lasso]\n",
    "    X_val_lasso = X_val.iloc[:, selected_features_lasso]\n",
    "    \n",
    "    # Fit the model and calculate R² scores\n",
    "    model_rfe.fit(X_train_lasso, y_train)\n",
    "    train_r2_score_lasso = model_rfe.score(X_train_lasso, y_train)\n",
    "    val_r2_score_lasso = model_rfe.score(X_val_lasso, y_val)\n",
    "    \n",
    "    print(f\"Lasso - alpha={alpha}:\")\n",
    "    print(f\"Training R² Score: {train_r2_score_lasso:.4f}, Validation R² Score: {val_r2_score_lasso:.4f}, Features Remaining: {len(selected_features_lasso)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results. Which features would you choose to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your written answer here\n",
    "RFE Results: You will observe how the training and validation ( R^2 ) scores change as you vary the number of features. If you notice that the validation score starts to decrease as you increase the number of features, it may indicate overfitting.\n",
    "\n",
    "Lasso Results: The Lasso regression results will show how different alpha values affect the number of selected features and the model's performance. A higher alpha typically results in fewer features being"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Final Model on Test Data\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "At the start of this lab, we created `X_test` and `y_test`. Prepare `X_test` the same way that `X_train` and `X_val` have been prepared. This includes scaling, adding interactions, and adding polynomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train, X_val, and X_test have already been defined\n",
    "# Combine train and validation sets for scaling and polynomial feature generation\n",
    "X_combined = pd.concat([X_train, X_val])  # Combine train and validation sets\n",
    "\n",
    "# Step 1: Scale the combined data\n",
    "scaler = StandardScaler()\n",
    "X_combined_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Step 2: Add polynomial features to the combined scaled data\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_combined_poly = poly.fit_transform(X_combined_scaled)\n",
    "\n",
    "# Step 3: Prepare X_test\n",
    "# Fill missing columns in X_test with zeros or NaNs\n",
    "for col in X_combined.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0  # or use NaN: X_test[col] = np.nan\n",
    "\n",
    "# Now you can proceed to scale and transform\n",
    "X_test_scaled = scaler.transform(X_test)  # Scale X_test\n",
    "X_test_poly = poly.transform(X_test_scaled)  # Generate polynomial features for X_test\n",
    "\n",
    "# Now X_test_poly is ready for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either `RFE` or `Lasso`, fit a model on the complete train + validation set, then find R-Squared and MSE values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² Score: 0.7536\n",
      "Test MSE: 1726415076.4056\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Combine training and validation sets\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# Scale and create polynomial features for the combined training and validation sets\n",
    "X_train_val_scaled = scaler.transform(X_train_val)\n",
    "X_train_val_poly = poly.transform(X_train_val_scaled)\n",
    "\n",
    "# Fit Lasso model on the combined training and validation sets\n",
    "lasso = LassoCV(cv=5)\n",
    "lasso.fit(X_train_val_poly, y_train_val)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso.predict(X_test_poly)\n",
    "\n",
    "# Calculate R-Squared and MSE\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test R² Score: {r_squared:.4f}\")\n",
    "print(f\"Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up Ideas (Optional)\n",
    "\n",
    "### Create a Lasso Path\n",
    "\n",
    "From this section, you know that when using `Lasso`, more parameters shrink to zero as your regularization parameter goes up. In scikit-learn there is a function `lasso_path()` which visualizes the shrinkage of the coefficients while $alpha$ changes. Try this out yourself!\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n",
    "\n",
    "### AIC and BIC for Subset Selection\n",
    "\n",
    "This notebook shows how you can use AIC and BIC purely for feature selection. Try this code out on our Ames housing data!\n",
    "\n",
    "https://xavierbourretsicotte.github.io/subset_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to apply concepts of bias-variance tradeoff using extensions to linear models and feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
